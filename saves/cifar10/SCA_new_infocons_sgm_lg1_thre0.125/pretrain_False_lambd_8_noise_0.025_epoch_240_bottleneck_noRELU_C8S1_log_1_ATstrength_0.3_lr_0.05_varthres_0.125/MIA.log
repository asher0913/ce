2025-06-19 01:20:02,057 DEBUG Split Learning Scheme: Overall Cutting_layer 4/13
2025-06-19 01:20:06,146 DEBUG Namespace(arch='vgg11_bn_sgm', cutlayer=4, batch_size=128, filename='pretrain_False_lambd_8_noise_0.025_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125', folder='saves/cifar10/SCA_new_infocons_sgm_lg1_thre0.125', num_client=1, num_epochs=240, learning_rate=0.05, lambd=8.0, dataset_portion=1.0, client_sample_ratio=1.0, noniid=1.0, local_lr=-1.0, dataset='cifar10', scheme='V2_epoch', regularization='Gaussian_kl', regularization_strength=0.025, var_threshold=0.125, AT_regularization='SCA_new', AT_regularization_strength=0.3, log_entropy=1.0, ssim_threshold=0.5, gan_AE_type='res_normN4C64', gan_loss_type='SSIM', bottleneck_option='noRELU_C8S1', optimize_computation=1, decoder_sync=False, load_from_checkpoint=False, load_from_checkpoint_server=False, transfer_source_task='cifar100', finetune_freeze_bn=False, save_more_checkpoints=False, initialize_different=False, random_seed=125)
2025-06-19 01:20:06,342 DEBUG Model's smashed-data size is torch.Size([1, 8, 8, 8])
2025-06-19 01:20:06,476 DEBUG Real Train Phase: done by all clients, for total 240 epochs
2025-06-19 01:20:06,477 DEBUG GAN training interval N (once every N step) is set to 1!
2025-06-19 01:20:06,478 DEBUG Train in V2_epoch style
2025-06-19 01:20:06,733 DEBUG log--[1/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 2.3266
2025-06-19 01:20:18,198 DEBUG log--[1/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 2.3213
2025-06-19 01:20:18,399 DEBUG Epoch 0	Test (client-0):	Loss 2.3120 (2.3120)	Prec@1 8.594 (8.594)
2025-06-19 01:20:18,836 DEBUG Epoch 50	Test (client-0):	Loss 2.3034 (2.3040)	Prec@1 17.188 (11.397)
2025-06-19 01:20:19,119 DEBUG  * Prec@1 11.170
2025-06-19 01:20:23,030 DEBUG the mean of mutal infor is:(-5.030), the est mean of mutal infor is:(-3.462)
2025-06-19 01:22:00,078 DEBUG Split Learning Scheme: Overall Cutting_layer 4/13

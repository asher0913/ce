2025-06-19 01:15:55,438 DEBUG Split Learning Scheme: Overall Cutting_layer 4/13
2025-06-19 01:16:00,770 DEBUG Namespace(arch='vgg11_bn_sgm', cutlayer=4, batch_size=128, filename='pretrain_False_lambd_0_noise_0.15_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125', folder='saves/cifar10/SCA_new_infocons_sgm_lg1_thre0.125', num_client=1, num_epochs=240, learning_rate=0.05, lambd=0.0, dataset_portion=1.0, client_sample_ratio=1.0, noniid=1.0, local_lr=-1.0, dataset='cifar10', scheme='V2_epoch', regularization='Gaussian_kl', regularization_strength=0.15, var_threshold=0.125, AT_regularization='SCA_new', AT_regularization_strength=0.3, log_entropy=1.0, ssim_threshold=0.5, gan_AE_type='res_normN4C64', gan_loss_type='SSIM', bottleneck_option='noRELU_C8S1', optimize_computation=1, decoder_sync=False, load_from_checkpoint=False, load_from_checkpoint_server=False, transfer_source_task='cifar100', finetune_freeze_bn=False, save_more_checkpoints=False, initialize_different=False, random_seed=125)
2025-06-19 01:16:00,973 DEBUG Model's smashed-data size is torch.Size([1, 8, 8, 8])
2025-06-19 01:16:01,108 DEBUG Real Train Phase: done by all clients, for total 240 epochs
2025-06-19 01:16:01,110 DEBUG GAN training interval N (once every N step) is set to 1!
2025-06-19 01:16:01,111 DEBUG Train in V2_epoch style
2025-06-19 01:16:01,410 DEBUG log--[1/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 2.3290
2025-06-19 01:16:11,023 DEBUG log--[1/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 2.3219
2025-06-19 01:16:11,238 DEBUG Epoch 0	Test (client-0):	Loss 2.3177 (2.3177)	Prec@1 9.375 (9.375)
2025-06-19 01:16:11,660 DEBUG Epoch 50	Test (client-0):	Loss 2.3080 (2.3048)	Prec@1 11.719 (10.892)
2025-06-19 01:16:11,932 DEBUG  * Prec@1 10.680
2025-06-19 01:16:15,550 DEBUG the mean of mutal infor is:(-3.313), the est mean of mutal infor is:(-2.896)
2025-06-19 01:17:48,349 DEBUG Split Learning Scheme: Overall Cutting_layer 4/13

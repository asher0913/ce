2025-06-19 01:24:05,746 DEBUG Split Learning Scheme: Overall Cutting_layer 4/13
2025-06-19 01:24:11,761 DEBUG Namespace(arch='vgg11_bn_sgm', cutlayer=4, batch_size=128, filename='pretrain_False_lambd_8_noise_0.1_epoch_240_bottleneck_noRELU_C8S1_log_1_ATstrength_0.3_lr_0.05_varthres_0.125', folder='saves/cifar10/SCA_new_infocons_sgm_lg1_thre0.125', num_client=1, num_epochs=240, learning_rate=0.05, lambd=8.0, dataset_portion=1.0, client_sample_ratio=1.0, noniid=1.0, local_lr=-1.0, dataset='cifar10', scheme='V2_epoch', regularization='Gaussian_kl', regularization_strength=0.1, var_threshold=0.125, AT_regularization='SCA_new', AT_regularization_strength=0.3, log_entropy=1.0, ssim_threshold=0.5, gan_AE_type='res_normN4C64', gan_loss_type='SSIM', bottleneck_option='noRELU_C8S1', optimize_computation=1, decoder_sync=False, load_from_checkpoint=False, load_from_checkpoint_server=False, transfer_source_task='cifar100', finetune_freeze_bn=False, save_more_checkpoints=False, initialize_different=False, random_seed=125)
2025-06-19 01:24:11,958 DEBUG Model's smashed-data size is torch.Size([1, 8, 8, 8])
2025-06-19 01:24:12,090 DEBUG Real Train Phase: done by all clients, for total 240 epochs
2025-06-19 01:24:12,091 DEBUG GAN training interval N (once every N step) is set to 1!
2025-06-19 01:24:12,091 DEBUG Train in V2_epoch style
2025-06-19 01:24:12,320 DEBUG log--[1/240][0/391][client-0] train loss: 0.0000 cross-entropy loss: 2.3291
2025-06-19 01:24:22,109 DEBUG log--[1/240][390/391][client-0] train loss: 0.0000 cross-entropy loss: 2.3217
2025-06-19 01:24:22,328 DEBUG Epoch 0	Test (client-0):	Loss 2.3143 (2.3143)	Prec@1 9.375 (9.375)
2025-06-19 01:24:22,778 DEBUG Epoch 50	Test (client-0):	Loss 2.3061 (2.3044)	Prec@1 14.062 (11.014)
2025-06-19 01:24:23,074 DEBUG  * Prec@1 10.720
2025-06-19 01:24:26,985 DEBUG the mean of mutal infor is:(-3.862), the est mean of mutal infor is:(-3.170)
2025-06-19 01:26:10,402 DEBUG Split Learning Scheme: Overall Cutting_layer 4/13
